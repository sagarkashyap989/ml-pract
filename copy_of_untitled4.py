# -*- coding: utf-8 -*-
"""Copy of Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-6FPx2cDCh6_ZXZDJYNT09VjHQk5suPC
"""

##Write a program to implement sentence segmentation and word
# tokenization

import nltk
from nltk.tokenize import sent_tokenize, word_tokenize

# Download silently and check if already available
nltk.download("punkt", quiet=True)

text = "sagar kashyap is here."

# Sentence segmentation
sentences = sent_tokenize(text)
print("Sentence Segmentation:")
for sentence in sentences:
    print(sentence)

# Word tokenization
for sentence in sentences:
    words = word_tokenize(sentence)
    print("\nWord Tokenization:", words)































# AIM: Write a program to Implement stemming and lemmatization.
#

from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import nltk

# Download required resources
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Stemming
stemmer = PorterStemmer()
text = "The striped bats are hanging on their feet for best"
words = word_tokenize(text)

print("Stemming using NLTK:")
for word in words:
    print(f"{word} --> {stemmer.stem(word)}")

# Lemmatization
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

print("\nLemmatization using NLTK:")
for word in words:
    print(f"{word} --> {lemmatizer.lemmatize(word)}")

# AIM: Write a program to Implement a tri-gram model.
#

import nltk
from nltk.util import ngrams
from nltk.tokenize import word_tokenize
from collections import Counter

# Download required resource
nltk.download('punkt')

text = "Natural Language Processing with Python is interesting and fun to learn."

# Tokenize the text
tokens = word_tokenize(text)

# Generate trigrams
trigrams = list(ngrams(tokens, 3))
print("Trigrams:")
for trigram in trigrams:
    print(trigram)

# Optional: Frequency count of trigrams
trigram_freq = Counter(trigrams)
print("\nTrigram Frequencies:")
for trigram, freq in trigram_freq.items():
    print(f"{trigram} : {freq}")

# AIM: Write a program to Implement PoS tagging using HMM & Neural Model.
#

import spacy

# Load the English model
nlp = spacy.load("en_core_web_sm")

text = "Natural Language Processing enables computers to understand human language."
doc = nlp(text)

# Display POS tags
print("PoS Tagging:")
for token in doc:
    print(f"{token.text} --> {token.pos_} ({token.tag_})")

#AIM: Write a program to Implement syntactic parsing of a given text.


import nltk
from nltk import CFG

# Download required resources
nltk.download('punkt')

# Define the grammar
grammar = CFG.fromstring("""
  S -> NP VP
  NP -> DT NN | DT JJ NN | PRP
  VP -> VBZ NP | VB NP | VB
  DT -> 'the' | 'a'
  JJ -> 'big' | 'small'
  NN -> 'dog' | 'cat' | 'food'
  VBZ -> 'eats' | 'likes'
  VB -> 'run' | 'play'
  PRP -> 'he' | 'she'
""")

# Create parser
parser = nltk.ChartParser(grammar)

# Input sentence
sentence = "the big dog eats the food"
tokens = nltk.word_tokenize(sentence)

# Parse and print tree(s)
print("Syntactic Parse Tree(s):")
for tree in parser.parse(tokens):
    print(tree)
    tree.pretty_print()

# AIM: Write a program to Implement dependency parsing of a given text.


import spacy
from spacy import displacy

# Load the English model
nlp = spacy.load("en_core_web_sm")

text = "The quick brown fox jumps over the lazy dog."

# Process the text
doc = nlp(text)

# Print dependency parsing results
print("Dependency Parsing (word --> dependency relation --> head word):")
for token in doc:
    print(f"{token.text} --> {token.dep_} --> {token.head.text}")

#AIM: Write a program to Implement Named Entity Recognition (NER).

import spacy
from spacy import displacy

# Load spaCy model
nlp = spacy.load("en_core_web_sm")

# Sample text
text = "Barack Obama was born in Hawaii and served as the president of the United States."
doc = nlp(text)

# Print named entities
print("Named Entities (spaCy):")
for ent in doc.ents:
    print(f"{ent.text} --> {ent.label_}")

#AIM: Write a program to Implement Text Summarization for the given sample
# text.


import spacy
from collections import Counter
import heapq

# Load spaCy model
nlp = spacy.load("en_core_web_sm")

# Define the text input
text = "Natural Language Processing is a fascinating field. Would you like me to refine or expand on any specific area of NLP?"

# Process the text
doc = nlp(text)

# Calculate word frequencies (excluding stop words and punctuation)
word_frequencies = {}
for word in doc:
    if word.text.lower() not in nlp.Defaults.stop_words and not word.is_punct:
        word_frequencies[word.text.lower()] = word_frequencies.get(word.text.lower(), 0) + 1

# Normalize frequencies
max_freq = max(word_frequencies.values(), default=1)
for word in word_frequencies:
    word_frequencies[word] /= max_freq

# Score sentences based on word frequencies
sentence_scores = {}
for sent in doc.sents:
    for word in sent:
        if word.text.lower() in word_frequencies:
            sentence_scores[sent] = sentence_scores.get(sent, 0) + word_frequencies[word.text.lower()]

# Get top 2 sentences as summary
summary_sentences = heapq.nlargest(2, sentence_scores, key=sentence_scores.get)
summary = ' '.join([sent.text for sent in summary_sentences])

print("\nCustom Extractive Summary:")
print(summary)



