# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bCVrDqUIDz9BASe_4svxSWcCCWiBxsol
"""

#Implement Linear Regression (Diabetes Datasets) 11111

# import libraries
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score

# Load diabetes dataset
diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)

# Use only one feature (the third one)
diabetes_X = diabetes_X[:, np.newaxis, 2]

# Split the data into training/testing sets
diabetes_X_train = diabetes_X[:-20]
diabetes_X_test = diabetes_X[-20:]

# Split the targets into training/testing sets
diabetes_y_train = diabetes_y[:-20]
diabetes_y_test = diabetes_y[-20:]

# Create linear regression object
regr = linear_model.LinearRegression()

# Train the model using the training sets
regr.fit(diabetes_X_train, diabetes_y_train)

# Make predictions using the testing set
diabetes_y_pred = regr.predict(diabetes_X_test)

# Print model performance
print('Coefficients:', regr.coef_)
print('Mean squared error: %.2f' % mean_squared_error(diabetes_y_test, diabetes_y_pred))
print('Coefficient of determination (R^2): %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))

# Plot outputs
plt.scatter(diabetes_X_test, diabetes_y_test, color='black')
plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)
plt.xticks(())
plt.yticks(())
plt.show()

#Implement Logistics Regression (Iris Dataset)  222222
# Importing required libraries
import numpy as np  # For numerical operations and array manipulation
import matplotlib.pyplot as plt  # For plotting graphs
from sklearn.linear_model import LogisticRegression  # For creating a logistic regression model
from sklearn import datasets  # For loading sample datasets like Iris

# Load the Iris dataset
iris = datasets.load_iris()  # Loads a dictionary-like object with data and labels

# Use only the first two features (sepal length and sepal width) for 2D visualization
X = iris.data[:, :2]  # Select all rows, and first two columns of data
Y = iris.target       # Target labels (0, 1, 2 for the 3 flower species)

# Create a Logistic Regression classifier
logreg = LogisticRegression(C=1e5)  # C is inverse of regularization strength; large C means less regularization

# Train the model on the selected features
logreg.fit(X, Y)  # Fit the model to the data (X = input features, Y = target labels)

# Plot the decision boundary by predicting class for each point on a mesh grid
x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5  # Range of sepal length
y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5  # Range of sepal width
h = 0.02  # Step size in the mesh

# Create a mesh grid of points within the defined ranges
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

# Predict the class for each point in the mesh
# np.c_[] stacks xx and yy to shape (num_points, 2) â€” needed for prediction
Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])

# Reshape Z to match the shape of xx for plotting
Z = Z.reshape(xx.shape)

# Create a figure for plotting
plt.figure(1, figsize=(4, 3))  # 4x3 inch figure

# Plot the decision boundary by coloring regions based on class
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)  # Color mesh according to predicted classes

# Plot the actual training points
plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors="k", cmap=plt.cm.Paired)  # c=Y assigns colors to points by class

# Label the axes
plt.xlabel("Sepal length")
plt.ylabel("Sepal width")

# Set the axis limits to the mesh boundaries
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())

# Remove tick marks for better visual

# Practical 3: Implements Multinomial Logistic Regression (Iris Dataset)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn import datasets

# Load the iris dataset
iris = datasets.load_iris()
X = iris.data[:, :2]  # Use only the first two features for 2D visualization
y = iris.target       # Target labels

# Loop over both multinomial and one-vs-rest logistic regression approaches
for multi_class in ("multinomial", "ovr"):
    # Create and train the logistic regression classifier
    clf = LogisticRegression(solver="sag", max_iter=1000, random_state=42, multi_class=multi_class).fit(X, y)

    # Print training score
    print("training score : %.3f (%s)" % (clf.score(X, y), multi_class))

    # Create mesh to plot decision boundaries
    h = 0.02  # step size in the mesh
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

    # Predict class for each point in the mesh
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    # Plot decision surface
    plt.figure()
    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)
    plt.title("Decision surface of LogisticRegression (%s)" % multi_class)
    plt.axis("tight")

    # Plot the training points
    colors = "bry"
    for i, color in zip(clf.classes_, colors):
        idx = np.where(y == i)
        plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired, edgecolor="black", s=20)

    # Set axis limits
    xmin, xmax = plt.xlim()
    ymin, ymax = plt.ylim()

    # Get coefficients and intercepts
    coef = clf.coef_
    intercept = clf.intercept_

    # Function to plot the hyperplane for class `c`
    def plot_hyperplane(c, color):
        def line(x0):
            return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]
        plt.plot([xmin, xmax], [line(xmin), line(xmax)], ls="--", color=color)

    # Plot hyperplanes for each class
    for i, color in zip(clf.classes_, colors):
        plot_hyperplane(i, color)

    plt.show()

# Practical 4: Implement SVM classifier (Iris Dataset)

import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets

# Function to create meshgrid from input data for contour plotting
def make_meshgrid(x, y, h=0.02):
    x_min, x_max = x.min() - 2, x.max() + 1
    y_min, y_max = y.min() - 1, y.max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    return xx, yy

# Function to plot decision boundary contours
def plot_contours(ax, clf, xx, yy, **params):
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])  # Predict over meshgrid points
    Z = Z.reshape(xx.shape)  # Reshape to fit mesh
    out = ax.contourf(xx, yy, Z, **params)
    return out

# Load iris dataset and use only first 2 features for 2D visualization
iris = datasets.load_iris()
X = iris.data[:, :2]  # Sepal length and width
y = iris.target       # Target classes

# Regularization parameter
C = 1.0

# Create a list of SVM models with different kernels
models = (
    svm.SVC(kernel="linear", C=C),
    svm.LinearSVC(C=C, max_iter=10000),
    svm.SVC(kernel="rbf", gamma=0.7, C=C),
    svm.SVC(kernel="poly", degree=3, gamma="auto", C=C),
)

# Fit each model to the data
models = (clf.fit(X, y) for clf in models)

# Titles for each subplot
titles = (
    "SVC with linear kernel",
    "LinearSVC (linear kernel)",
    "SVC with RBF kernel",
    "SVC with polynomial (degree 3) kernel",
)

# Create subplots grid
fig, sub = plt.subplots(2, 2)
plt.subplots_adjust(wspace=0.4, hspace=0.4)

# Create meshgrid for plotting
X0, X1 = X[:, 0], X[:, 1]
xx, yy = make_meshgrid(X0, X1)

# Plot each classifier
for clf, title, ax in zip(models, titles, sub.flatten()):
    plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)
    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors="k")
    ax.set_xlim(xx.min(), xx.max())
    ax.set_ylim(yy.min(), yy.max())
    ax.set_xlabel("Sepal length")
    ax.set_ylabel("Sepal width")
    ax.set_xticks(())
    ax.set_yticks(())
    ax.set_title(title)

# Show final plot
plt.show()

# Practical 5: Train and fine-tune a Decision Tree for the Moons Dataset

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score

# Generate a synthetic dataset: two interleaving half circles (moons)
X, y = make_moons(n_samples=200, noise=0.25, random_state=42)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create a Decision Tree classifier (initial model)
dt_clf = DecisionTreeClassifier(random_state=42)

# Define a parameter grid to fine-tune the tree
param_grid = {
    "max_depth": [1, 2, 3, 4, 5, 6],
    "min_samples_split": [2, 4, 6],
    "criterion": ["gini", "entropy"]
}

# Use GridSearchCV to find the best combination of hyperparameters
grid_search = GridSearchCV(dt_clf, param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Best model after tuning
best_tree = grid_search.best_estimator_

# Predict on test data
y_pred = best_tree.predict(X_test)

# Accuracy of the best model
print("Best Parameters:", grid_search.best_params_)
print("Test Accuracy: %.2f%%" % (accuracy_score(y_test, y_pred) * 100))

# Function to plot decision boundary
def plot_decision_boundary(model, X, y, title):
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),
                         np.linspace(y_min, y_max, 300))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(8, 6))
    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolors='k')
    plt.title(title)
    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    plt.show()

# Plot the decision boundary of the best model
plot_decision_boundary(best_tree, X, y, "Decision Tree (Best Parameters)")

# Practical 6: Train an SVM regressor on the California Housing Dataset

import pandas as pd
import numpy as np
from sklearn.datasets import fetch_california_housing  # To load housing dataset

# Load the California Housing dataset
housing = fetch_california_housing()
X = housing.data     # Features (e.g., income, house age, etc.)
y = housing.target   # Target (median house value)

# Split the data into training and testing sets (75% train, 25% test)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Standardize the feature values for better SVM performance
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

# Fit the scaler only on training data and transform both train and test data
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Import and create a Linear Support Vector Regressor
from sklearn.svm import LinearSVR
lin_svr = LinearSVR(random_state=42)

# Train the model on scaled training data
lin_svr.fit(X_train_scaled, y_train)

# Predict the target values for the training data
from sklearn.metrics import mean_squared_error
y_pred = lin_svr.predict(X_train_scaled)

# Calculate Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)
mse = mean_squared_error(y_train, y_pred)
print("MSE:", mse)
print("RMSE:", np.sqrt(mse))

#Practical 7: Implement Batch Gradient Descent with early stopping for Softmax
#Regression

import numpy as np

# Softmax function for probability output
def softmax(z):
    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # numerical stability
    return exp_z / np.sum(exp_z, axis=1, keepdims=True)

# Cross-entropy loss function
def compute_loss(y_true, y_pred):
    n_samples = y_true.shape[0]
    log_likelihood = -np.log(y_pred[range(n_samples), y_true])
    return np.mean(log_likelihood)

# Batch Gradient Descent with Early Stopping for Softmax Regression
def batch_gradient_descent(X, y, learning_rate=0.01, max_epochs=1000, tol=1e-4, patience=10):
    n_samples, n_features = X.shape
    n_classes = np.max(y) + 1  # Assuming y contains class labels 0,1,...,k

    # Initialize weights and biases
    weights = np.zeros((n_features, n_classes))
    biases = np.zeros((1, n_classes))

    best_loss = float('inf')
    patience_counter = 0

    for epoch in range(max_epochs):
        logits = np.dot(X, weights) + biases  # Linear output
        y_pred = softmax(logits)              # Softmax output

        # Compute current loss
        loss = compute_loss(y, y_pred)

        # Early stopping logic
        if loss < best_loss - tol:
            best_loss = loss
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"Early stopping at epoch {epoch}")
                break

        # One-hot encode true labels
        y_one_hot = np.eye(n_classes)[y]

        # Compute gradients
        gradient_weights = np.dot(X.T, (y_pred - y_one_hot)) / n_samples
        gradient_biases = np.sum(y_pred - y_one_hot, axis=0, keepdims=True) / n_samples

        # Update weights and biases
        weights -= learning_rate * gradient_weights
        biases -= learning_rate * gradient_biases

        # Optional: print every 100 epochs
        if epoch % 100 == 0 or epoch == max_epochs - 1:
            print(f"Epoch {epoch}, Loss: {loss:.4f}")

    return weights, biases

# Example usage
if __name__ == "__main__":
    # Dummy dataset for demonstration
    np.random.seed(42)
    X = np.random.rand(100, 3)               # 100 samples, 3 features
    y = np.random.randint(0, 3, 100)         # 3 classes (0, 1, 2)

    # Train softmax regression
    weights, biases = batch_gradient_descent(
        X, y, learning_rate=0.1, max_epochs=1000, tol=1e-4, patience=10
    )

    print("Training complete.")

# Practical 8: Implement MLP for classification of handwritten digits (MNIST Dataset)

import matplotlib.pyplot as plt
from sklearn.datasets import fetch_openml
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, accuracy_score
import numpy as np

# Load the MNIST dataset (70,000 images of 28x28 pixels)
X, y = fetch_openml("mnist_784", version=1, return_X_y=True, as_frame=False)

# Normalize pixel values to the range [0, 1]
X = X / 255.0

# Check the shape of input data
print("Dataset shape:", X.shape)  # Should be (70000, 784)

# Split into training and testing sets (first 60K for training, last 10K for testing)
X_train, X_test = X[:60000], X[60000:]
y_train, y_test = y[:60000], y[60000:]

# Define MLP model with 3 hidden layers: 50, 20, and 10 neurons respectively
classifier = MLPClassifier(
    hidden_layer_sizes=(50, 20, 10),
    max_iter=100,              # Number of epochs
    alpha=1e-4,                # L2 penalty (regularization)
    solver="sgd",              # Stochastic Gradient Descent
    verbose=10,                # Print progress
    random_state=1,
    learning_rate_init=0.1     # Initial learning rate
)

# Train the model
classifier.fit(X_train, y_train)

# Predict on test data
y_pred = classifier.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("\nTest Accuracy: %.2f%%" % (accuracy * 100))

# Show classification report
print("\nClassification Report:\n")
print(classification_report(y_test, y_pred))

# Optional: visualize some predictions
fig, axes = plt.subplots(2, 5, figsize=(10, 5))
for i, ax in enumerate(axes.ravel()):
    ax.imshow(X_test[i].reshape(28, 28), cmap='gray')
    ax.set_title(f"True: {y_test[i]}\nPred: {y_pred[i]}")
    ax.axis('off')
plt.tight_layout()
plt.show()